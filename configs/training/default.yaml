# Training configuration
# Override with: training.max_epochs=50 training.lr=1e-3

# Optimizer
optimizer:
  type: adamw
  lr: 1e-4
  weight_decay: 1e-4
  betas: [0.9, 0.999]

# Learning rate scheduler
scheduler:
  type: cosine_annealing
  T_max: ${training.max_epochs}
  eta_min: 1e-6

# Loss function
loss:
  type: bce_dice  # Options: bce, dice, bce_dice, focal_dice
  bce_weight: 0.5
  dice_weight: 0.5
  # Focal loss params (used if type contains "focal")
  focal_alpha: 0.25
  focal_gamma: 2.0

# Training settings
max_epochs: 100
accumulate_grad_batches: 2  # Effective batch size = batch_size * accumulate
gradient_clip_val: 1.0
precision: "16-mixed"  # Mixed precision for faster training

# Early stopping
early_stopping:
  enabled: true
  monitor: val/dice
  patience: 15
  mode: max
  min_delta: 0.001

# Model checkpointing
checkpoint_dir: checkpoints
checkpointing:
  monitor: val/dice
  save_top_k: 3
  mode: max
  save_last: true

# Validation
val_check_interval: 1.0  # Validate every epoch
check_val_every_n_epoch: 1

# Logging
logging:
  project: grave-detection
  log_every_n_steps: 10
  log_images_every_n_epochs: 5

# Fast dev run (for testing)
fast_dev_run: false  # Set to true or int for quick test
